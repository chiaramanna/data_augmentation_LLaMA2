{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for preprocessing and augmented text datasets.\n",
    "\n",
    "This script performs the following steps:\n",
    "1. As the data augmentation in LLAMA2_augmentation_classification.ipynb results in different files for each class,\n",
    "    this script concatenates multiple CSV files for each category (violent crime, cybercrime, weapons trade, drugs trade).\n",
    "2. Removes duplicate rows and text duplicates within each category.\n",
    "3. Filters out rows where the text starts with specific patterns (e.g., 'Here') for further inspection.\n",
    "    --> This part is manual, as it requires some inspection, the aim is to filter out instances were the Desired Format (see prompt)\n",
    "        is repeated, or similar patterns like 'Here is one __ example'\n",
    "4. Cleans the text data by removing phone numbers, email addresses, non-ASCII characters, and uncommon punctuation.\n",
    "5. Divides the cleaned datasets into train and trainval augmented sets based on provided train.csv and trainval.csv files.\n",
    "7. Saves the resulting datasets as CSV files in the 'processed_datasets' folder for each category.\n",
    "\n",
    "Usage:\n",
    "- Make sure to customize the file paths, prefixes, and file numbers according to your dataset structure.\n",
    "- Ensure that the 'train.csv' and 'trainval.csv' files are available for merging.\n",
    "\n",
    "Note: The script assumes that the columns 'text', 'label', and 'snapshot_id' are present in the original datasets.\n",
    "      The functions need to be called per each class individually.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(prefix: str, num_files: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenates multiple CSV files into a single DataFrame for a given prefix and number of files.\n",
    "\n",
    "    Parameters:\n",
    "    - prefix (str): The location of the CSV files.\n",
    "    - num_files (int): The number of CSV files to concatenate.\n",
    "\n",
    "    It assumes that the files are saved with a prefix and number, as suggested in LLAMA2_augmentation_classification.ipynb\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing concatenated data from all specified files.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        df = pd.read_csv(f\"{prefix}{i}.csv\")\n",
    "        df_filtered = df[df['is synthetic'] == True][['text', 'label', 'snapshot_id']].dropna().copy() \n",
    "        #augmented sets comprehend both original and synthetic examples, in this stage we filter only the synthetic ones\n",
    "        df_list.append(df_filtered)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_prefix = \"datasets_snapshot_id/aug_violent_crime_\"\n",
    "#num_files = 35\n",
    "df_aug_violent_crime = concat_files(file_prefix, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_prefix = \"datasets_snapshot_id/aug_cybercrime_\"\n",
    "#num_files = 4\n",
    "df_aug_cybercrime = concat_files(file_prefix, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_prefix = \"datasets_snapshot_id/aug_weapons_trade_\"\n",
    "#num_files = 9\n",
    "df_aug_weapons_trade = concat_files(file_prefix, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_prefix = \"datasets_snapshot_id/aug_drugs_trade_\"\n",
    "#num_files = 2\n",
    "df_aug_drugs_trade = concat_files(file_prefix, num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    duplicates = df.duplicated()\n",
    "    df_cleaned = df[~duplicates].copy()\n",
    "    return df_cleaned\n",
    "\n",
    "def remove_text_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    duplicates = df['text'].duplicated()\n",
    "    df_cleaned = df[~duplicates].copy()\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_violent_crime = remove_duplicates(df_aug_violent_crime)\n",
    "df_aug_cybercrime = remove_duplicates(df_aug_cybercrime)\n",
    "df_aug_weapons_trade = remove_duplicates(df_aug_weapons_trade)\n",
    "df_aug_drugs_trade = remove_duplicates(df_aug_drugs_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_violent_crime = remove_text_duplicates(df_aug_violent_crime)\n",
    "df_aug_cybercrime = remove_text_duplicates(df_aug_cybercrime)\n",
    "df_aug_weapons_trade = remove_text_duplicates(df_aug_weapons_trade)\n",
    "df_aug_drugs_trade = remove_text_duplicates(df_aug_drugs_trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_start_texts(df: pd.DataFrame, start_texts: List[str]) -> pd.DataFrame:\n",
    "    start_texts_lower = [start_text.lower() for start_text in start_texts]\n",
    "    condition = df['text'].str[:4].str.lower().isin(start_texts_lower)\n",
    "    return df[condition]\n",
    "#Text entries starting with 'here' are inspected manually, and the index eventually dropped\n",
    "\n",
    "start_texts = ['Here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_start_texts(df_aug_violent_crime, start_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aug_violent_crime = df_aug_violent_crime.drop([69, 461, 569]).reset_index(drop=True)\n",
    "#Insert indexed of entries to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_start_texts(df_aug_cybercrime, start_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aug_cybercrime = df_aug_cybercrime.drop([130]).reset_index(drop= True)\n",
    "#Insert indexed of entries to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_start_texts(df_aug_weapons_trade, start_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aug_weapons_trade = df_aug_weapons_trade.drop([102, 139, 331, 353, 433, 602, 714, 773]).reset_index(drop=True)\n",
    "#Insert indexed of entries to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_start_texts(df_aug_drugs_trade, start_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aug_drugs_trade = df_aug_drugs_trade.drop([136, 162, 163, 214, 254, 278, 425, 432, 446]).reset_index(drop=True)\n",
    "#Insert indexed of entries to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend original preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs text cleaning on a DataFrame containing a 'text' column.\n",
    "\n",
    "    The cleaning process includes:\n",
    "    1. Removing rows with less than 10 words.\n",
    "    2. Removing phone numbers.\n",
    "    3. Removing email addresses.\n",
    "    4. Removing non-ASCII characters.\n",
    "    5. Removing uncommon punctuation.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame with a 'text' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with cleaned 'text' column.\n",
    "    \"\"\"\n",
    "   \n",
    "    df['text_len'] = df['text'].apply(lambda x:len(x.split(' ')))\n",
    "\n",
    "    df = df[df.text_len >= 10].copy()\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub(r'\\b(?:\\+\\d{1,2}\\s?)?\\(?(?:\\d{1,4})?\\)?[-.\\s]?\\d{1,5}[-.\\s]?\\d{1,5}[-.\\s]?\\d{1,9}\\b', '', text))\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '', text))\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub(r'[^\\x00-\\x7F]+', '', text))\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub(r'[^\\w\\s.?!,:;\\'\"\\d-]', '', text))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_aug_violent_crime_pro = clean_text(df_aug_violent_crime)\n",
    "df_aug_weapons_trade_pro = clean_text(df_aug_weapons_trade)\n",
    "df_aug_cybercrime_pro = clean_text(df_aug_cybercrime)\n",
    "df_aug_drugs_trade_pro = clean_text(df_aug_drugs_trade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following lines of code are aimed at dividing the overall processed augmented sets into training and trainval sets,\n",
    "based on the 'snapshot_id', which identifies the original entry that was augmented. \n",
    "\n",
    "This is achieved by merging each augmented set with the original training set on snapshot_id and subsetting instances present in both sets.\n",
    "\n",
    "These steps are repeated for each class. Finally, each training set is aggregated into the final set of augmented entries, by sampling the number of entries needed to reach the majority class.\n",
    "\n",
    "Specify the path (based on configuration) and uncomment the line if saving is desired.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide into train and trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', index_col=0)\n",
    "df_trainval = pd.read_csv('trainval.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_violent_crime = pd.merge(df_aug_violent_crime_pro, df_train, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_train_violent_crime = df_merge_violent_crime[df_merge_violent_crime._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_train_violent_crime.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_train_violent_crime.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cybercrime = pd.merge(df_aug_cybercrime_pro, df_train, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_train_cybercrime = df_merge_cybercrime[df_merge_cybercrime._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_train_cybercrime.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_train_cybercrime.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_weapons_trade = pd.merge(df_aug_weapons_trade_pro, df_train, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_train_weapons_trade = df_merge_weapons_trade[df_merge_weapons_trade._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_train_weapons_trade.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_train_weapons_trade.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_drugs_trade = pd.merge(df_aug_drugs_trade_pro, df_train, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_train_drugs_trade = df_merge_drugs_trade[df_merge_drugs_trade._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_train_drugs_trade.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_train_drugs_trade.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_violent_crime = pd.merge(df_aug_violent_crime_pro, df_trainval, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_trainval_violent_crime = df_merge_violent_crime[df_merge_violent_crime._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_trainval_violent_crime.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_trainval_violent_crime.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cybercrime = pd.merge(df_aug_cybercrime_pro, df_trainval, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_trainval_cybercrime = df_merge_cybercrime[df_merge_cybercrime._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_trainval_cybercrime.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_trainval_cybercrime.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_weapons_trade = pd.merge(df_aug_weapons_trade_pro, df_trainval, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_trainval_weapons_trade = df_merge_weapons_trade[df_merge_weapons_trade._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_trainval_weapons_trade.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_trainval_weapons_trade.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_drugs_trade = pd.merge(df_aug_drugs_trade_pro, df_trainval, how='left', on='snapshot_id', indicator=True)\n",
    "df_aug_trainval_drugs_trade = df_merge_drugs_trade[df_merge_drugs_trade._merge == 'both'][['text_x', 'label_x', 'snapshot_id']]\n",
    "df_aug_trainval_drugs_trade.columns = [['text', 'label', 'snapshot_id']]\n",
    "df_aug_trainval_drugs_trade.shape\n",
    "#Check that shape is at least the required volume to reach the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate into final set of augmented entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_train_violent_crime = df_aug_train_violent_crime.sample(n=430, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_train_weapons_trade = df_aug_train_weapons_trade.sample(n=400, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_train_drugs_trade = df_aug_train_drugs_trade.sample(n=206, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_train_cybercrime = df_aug_train_cybercrime.sample(n=116, random_state=123).reset_index(drop=True).copy()\n",
    "\n",
    "augmented_train = pd.concat([df_aug_train_violent_crime, df_aug_train_weapons_trade, df_aug_train_drugs_trade, df_aug_train_cybercrime], ignore_index=True)\n",
    "#augmented_train.to_csv('augmented_train.csv')\n",
    "#Name it according to the configuration\n",
    "#Uncommenting it will overwrite the information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_trainval_violent_crime = df_aug_trainval_violent_crime.sample(n=575, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_trainval_weapons_trade = df_aug_trainval_weapons_trade.sample(n=535, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_trainval_drugs_trade = df_aug_trainval_drugs_trade.sample(n=275, random_state=123).reset_index(drop=True).copy()\n",
    "df_aug_trainval_cybercrime = df_aug_trainval_cybercrime.sample(n=155, random_state=123).reset_index(drop=True).copy()\n",
    "\n",
    "augmented_trainval = pd.concat([df_aug_trainval_violent_crime, df_aug_trainval_weapons_trade, df_aug_trainval_drugs_trade, df_aug_trainval_cybercrime], ignore_index=True)\n",
    "#augmented_trainval.to_csv('augmented_trainval.csv')\n",
    "#Name it according to the configuration\n",
    "#Uncommenting it will overwrite the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing samples for few-shot augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code prepares samples for few-shot augmentation by selecting examples from the original training set and their corresponding augmented counterparts for specific categories (violent crime, cybercrime, weapons trade, drugs trade). \n",
    "\n",
    "The process involves the following steps:\n",
    "\n",
    "1. Load Datasets:\n",
    "   - Load the original training dataset.\n",
    "   - Load the augmented datasets for different categories.\n",
    "\n",
    "2. Sample Examples:\n",
    "   - Sample a specified number (15) of examples from each augmented category to use in few-shot augmentation.\n",
    "\n",
    "3. Merge Samples with Original Training Examples:\n",
    "   - For each category, merge the sampled synthetic examples with their corresponding examples from the original training set based on the 'snapshot_id'.\n",
    "   - Rename columns to distinguish between synthetic and original examples.\n",
    "\n",
    "4. Optional: Save Samples to CSV:\n",
    "   - The resulting samples, containing both synthetic and original examples, can be saved to CSV files.\n",
    "     - Uncomment the relevant lines to save samples for each category to your preferred path.\n",
    "\n",
    "The goal is to create datasets for few-shot augmentation, where examples from the original training set serve as input demonstrations, and the corresponding augmented examples serve as output demonstrations.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('train.csv', index_col=0) #for the FS, we only sample example from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aug_train = pd.read_csv('augmented_train_quant.csv', index_col=0) #Specify path to augmented dataset\n",
    "df_aug_violent_crime = df_aug_train[df_aug_train.label == 'Violent Crime'].copy()\n",
    "df_aug_drugs_trade = df_aug_train[df_aug_train.label == 'Drugs and Narcotics Trade'].copy()\n",
    "df_aug_cybercrime = df_aug_train[df_aug_train.label == 'Cybercrime'].copy()\n",
    "df_aug_weapons_trade = df_aug_train[df_aug_train.label == 'Weapons Trade'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_violent_crime_train = df_aug_violent_crime.sample(n=15, random_state=123)\n",
    "sample_violent_crime_all = pd.merge(sample_violent_crime_train[['text', 'snapshot_id', 'label']], df_train[['text', 'snapshot_id']], on= 'snapshot_id', how='left', suffixes=('_synthetic', '_original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cybercrime_train = df_aug_cybercrime.sample(n=15, random_state=123)\n",
    "sample_cybercrime_all = pd.merge(sample_cybercrime_train[['text', 'snapshot_id', 'label']], df_train[['text', 'snapshot_id']], on= 'snapshot_id', how='left', suffixes=('_synthetic', '_original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weapons_trade_train = df_aug_weapons_trade.sample(n=15, random_state=123)\n",
    "sample_weapons_trade_all = pd.merge(sample_weapons_trade_train[['text', 'snapshot_id', 'label']], df_train[['text', 'snapshot_id']], on= 'snapshot_id', how='left', suffixes=('_synthetic', '_original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_drugs_trade_train = df_aug_drugs_trade.sample(n=15, random_state=123)\n",
    "sample_drugs_trade_all = pd.merge(sample_drugs_trade_train[['text', 'snapshot_id', 'label']], df_train[['text', 'snapshot_id']], on= 'snapshot_id', how='left', suffixes=('_synthetic', '_original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_violent_crime_all.to_csv('zs_sample_violent_crime_quant.csv')\n",
    "#sample_cybercrime_all.to_csv('zs_sample_cybercrime_quant.csv')\n",
    "#sample_weapons_trade_all.to_csv('zs_sample_weapons_trade_quant.csv')\n",
    "#sample_drugs_trade_all.to_csv('zs_sample_drugs_trade_quant.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
